{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f743ddc9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from data.ipynb\n",
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "inputs: (None, 96, 96, 1)\n",
      "conv1 (None, 96, 96, 32)\n",
      "pool1 (None, 48, 48, 32)\n",
      "pool1 (None, 48, 48, 32)\n",
      "conv2 (None, 48, 48, 64)\n",
      "pool2 (None, 24, 24, 64)\n",
      "pool2 (None, 24, 24, 64)\n",
      "conv3 (None, 24, 24, 128)\n",
      "pool3 (None, 12, 12, 128)\n",
      "pool3 (None, 12, 12, 128)\n",
      "conv4 (None, 12, 12, 256)\n",
      "pool4 (None, 6, 6, 256)\n",
      "pool4 (None, 6, 6, 256)\n",
      "conv5 (None, 6, 6, 512)\n",
      "conv5 (None, 6, 6, 512)\n",
      "KerasTensor(type_spec=TensorSpec(shape=(4,), dtype=tf.int32, name=None), inferred_value=[None, 6, 6, 512], name='tf.compat.v1.shape/Shape:0', description=\"created by layer 'tf.compat.v1.shape'\")\n",
      "value of q 1.31\n",
      "new_p (None, 6, 6, 512)\n",
      "xyz= (None, 1, 1, 512)\n",
      "xyz= (None, 1, 1, 512)\n",
      "weights (None, 6, 6, 512)\n",
      "improved_p= (None, 6, 6, 512)\n",
      "improved_pv2= <class 'tensorflow.python.keras.engine.keras_tensor.KerasTensor'> (None, 512)\n",
      "below dense KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense/Softmax:0', description=\"created by layer 'dense'\")\n",
      "data type <class 'float'> <class 'float'>\n",
      "type of data 32 <class 'int'>\n",
      "after_conv4 (None, 12, 12, 32)\n",
      "conv6 (None, 12, 12, 256)\n",
      "conv6 (None, 12, 12, 256)\n",
      "data type <class 'float'> <class 'float'>\n",
      "type of data 32 <class 'int'>\n",
      "after_conv3 (None, 24, 24, 32)\n",
      "conv7 (None, 24, 24, 128)\n",
      "conv7 (None, 24, 24, 128)\n",
      "data type <class 'float'> <class 'float'>\n",
      "type of data 32 <class 'int'>\n",
      "after_conv2 (None, 48, 48, 32)\n",
      "conv8 (None, 48, 48, 64)\n",
      "conv8 (None, 48, 48, 64)\n",
      "data type <class 'float'> <class 'float'>\n",
      "type of data 32 <class 'int'>\n",
      "after_conv1 (None, 96, 96, 32)\n",
      "conv9 (None, 96, 96, 32)\n",
      "conv9 (None, 96, 96, 32)\n",
      "conv10 (None, 96, 96, 1)\n",
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[64,96,96,96] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/concatenate_11/concat (defined at <ipython-input-1-695a28f923a4>:362) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_16891]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-695a28f923a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 410\u001b[1;33m    \u001b[0mtrain_and_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-695a28f923a4>\u001b[0m in \u001b[0;36mtrain_and_predict\u001b[1;34m()\u001b[0m\n\u001b[0;32m    360\u001b[0m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Fitting model...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m    \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m    model.fit(imgs_train, y=[imgs_mask_train,imgs_value_train], batch_size=64, epochs= 40, verbose=1, shuffle=True,\n\u001b[0m\u001b[0;32m    363\u001b[0m              validation_split=0.2 ,  callbacks=[model_checkpoint,early_stopping])\n\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[1;31m# stateless function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    951\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m       \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[64,96,96,96] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node model/concatenate_11/concat (defined at <ipython-input-1-695a28f923a4>:362) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_16891]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    " from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import import_ipynb\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imsave\n",
    "from data import load_test_data , load_train_data\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import ELU, LeakyReLU,UpSampling2D, Input,add, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose,Dense , Dropout, BatchNormalization, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.activations import softmax\n",
    "#from keras.layers.advanced_activations import ELU, LeakyReLU\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "K.set_image_data_format('channels_last')  # TF dimension ordering in this code\n",
    " \n",
    "img_rows = 96\n",
    "img_cols = 96\n",
    " \n",
    "smooth = 1.\n",
    "\n",
    "\n",
    "def NConv2D(filters, kernel_size, strides=(1, 1), padding='valid', dilation_rate=1,\n",
    "            activation=None, kernel_initializer='glorot_uniform'):\n",
    "    \n",
    "    \n",
    "    actv = activation == 'relu' and (lambda: LeakyReLU(0.0)) or activation == 'elu' and (lambda: ELU(1.0)) or None\n",
    "\n",
    "    def f(_input):\n",
    "        conv = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                      dilation_rate=dilation_rate, kernel_initializer=kernel_initializer)(_input)\n",
    "        norm = BatchNormalization(axis=3)(conv)\n",
    "        return actv()(norm)\n",
    "\n",
    "    return f\n",
    "\n",
    "def inception_block_v2(inputs, filters, activation=None, version='b', pars={}, allowed_pars={}):\n",
    "   \n",
    "    actv = activation == 'relu' and (lambda: LeakyReLU(0.0)) or activation == 'elu' and (lambda: ELU(1.0)) or None\n",
    "\n",
    "    c1_1 = Conv2D(filters=filters // 16, kernel_size=(1, 1), padding='same',\n",
    "                  activation=activation, kernel_initializer='he_normal')(inputs)\n",
    "    \n",
    "    c1_2 = NConv2D(filters=filters // 8, kernel_size=(1, 3), padding='same',\n",
    "                   activation=activation, kernel_initializer='he_normal')(c1_1)\n",
    "    c1_3 = NConv2D(filters=filters // 8, kernel_size=(3, 1), padding='same',\n",
    "                   activation=activation, kernel_initializer='he_normal')(c1_2)\n",
    "    c1_4 = NConv2D(filters=filters // 8, kernel_size=(1, 3), padding='same',\n",
    "                   activation=activation, kernel_initializer='he_normal')(c1_3)\n",
    "    \n",
    "    c1 = Conv2D(filters=filters // 8, kernel_size=(3, 1), padding='same', kernel_initializer='he_normal')(c1_4)\n",
    "    \n",
    "\n",
    "    # vertical 2\n",
    "    c2_1 = Conv2D(filters=filters // 8 * 3, kernel_size=(1, 1), padding='same',\n",
    "                  activation=activation, kernel_initializer='he_normal')(inputs)\n",
    "        \n",
    "    c2_2 = NConv2D(filters=filters // 2, kernel_size=(1, 3), padding='same',\n",
    "                   activation=activation, kernel_initializer='he_normal')(c2_1)\n",
    "                  \n",
    "    c2 = Conv2D(filters=filters // 2, kernel_size=(3, 1), padding='same', kernel_initializer='he_normal')(c2_2)\n",
    "    \n",
    "\n",
    "    # vertical 3\n",
    "    p3_1 = MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    c3 = Conv2D(filters=filters // 8, kernel_size=(1, 1), padding='same', kernel_initializer='he_normal')(p3_1)\n",
    "\n",
    "    # vertical 4\n",
    "    c4 = Conv2D(filters=filters // 4, kernel_size=(1, 1), padding='same', kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    # concatenating verticals together, normalizing and applying activation\n",
    "    result = concatenate([c1, c2, c3, c4], axis=3)\n",
    "    result = BatchNormalization(axis=3)(result)\n",
    "    result = actv()(result)\n",
    "    return result\n",
    "                  \n",
    "                  \n",
    "def pooling_block(inputs, filters, kernel_size=(3, 3), strides=(2, 2), padding='same', activation=None,\n",
    "                  pool_size=(2, 2), trainable=True, pars={}, allowed_pars={}):\n",
    "    \n",
    "    # checking that the allowed trainable parameters did not change in ALLOWED_PARS\n",
    "   \n",
    "    # keep trainable argument if need to use without PARS\n",
    "    \n",
    "\n",
    "    # setting the version from pars\n",
    "    \n",
    "\n",
    "    # returning block's output\n",
    "    if trainable:\n",
    "        return NConv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "                       padding=padding, activation=activation)(inputs)\n",
    "    else:\n",
    "        return MaxPooling2D(pool_size=pool_size, padding=padding)(inputs)\n",
    "\n",
    "\n",
    "def information_block(inputs, filters, kernel_size=(3, 3), padding='valid', activation=None,\n",
    "                      block='inception', block_type='v2', version='b', pars={}, allowed_pars={}):\n",
    "    \n",
    "    # getting which block, block_type, version to use as the information block\n",
    "    \n",
    "\n",
    "    # inception block\n",
    " \n",
    "    return inception_block_v2(inputs=inputs, filters=filters, activation=activation,\n",
    "                                      version=version, pars=pars, allowed_pars=allowed_pars)\n",
    "                  \n",
    "def _shortcut(_input, residual):\n",
    "    stride_width = _input.shape[1] / residual.shape[1]\n",
    "    stride_height = _input.shape[2] / residual.shape[2]\n",
    "    equal_channels = residual.shape[3] == _input.shape[3]\n",
    "\n",
    "    shortcut = _input\n",
    "    x = residual.shape[3]\n",
    "    print('data type',type(stride_width),type(stride_height))\n",
    "    print(\"type of data\",x,type(x))\n",
    "    # 1 X 1 conv if shape is different. Else identity.\n",
    "    if stride_width > 1 or stride_height > 1 or not equal_channels:\n",
    "        shortcut = Conv2D(filters=x, kernel_size=(1, 1),\n",
    "                          strides=(int(stride_width), int(stride_height)),\n",
    "                          kernel_initializer=\"he_normal\", padding=\"valid\")(_input)\n",
    "\n",
    "    return add([shortcut, residual])\n",
    "                  \n",
    "                  \n",
    "\n",
    "def rblock(inputs, filters, kernel_size, padding='valid', activation=None, scale=0.1):\n",
    "    \n",
    "    actv = activation == 'relu' and (lambda: LeakyReLU(0.0)) or activation == 'elu' and (lambda: ELU(1.0)) or None\n",
    "\n",
    "    residual = Conv2D(filters=filters, kernel_size=kernel_size, padding=padding)(inputs)\n",
    "    residual = BatchNormalization(axis=3)(residual)\n",
    "    residual = Lambda(lambda x: x * scale)(residual)\n",
    "    res = _shortcut(inputs, residual)\n",
    "    return actv()(res)\n",
    "        \n",
    "  \n",
    "\n",
    "def connection_block(inputs, filters, padding='valid', activation=None,\n",
    "                     version='residual', pars={}, allowed_pars={}):\n",
    "  \n",
    "    return rblock(inputs=inputs, filters=32, kernel_size=(1, 1), padding='same', activation=activation)\n",
    "    \n",
    "                  \n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    \n",
    " \n",
    " \n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)                  \n",
    "\n",
    "\n",
    "def get_unet(pars,allowed_pars):\n",
    "\n",
    "\n",
    "    inputs = Input((img_rows, img_cols, 1))\n",
    "    print('inputs:', inputs.shape)\n",
    "\n",
    "\n",
    "    conv1 = information_block(inputs, 32, padding='same', activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('conv1', conv1.shape)\n",
    "    pool1 = pooling_block(inputs=conv1, filters=32, activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('pool1', pool1.shape)\n",
    "    pool1 = Dropout(0.5)(pool1)\n",
    "    print('pool1', pool1.shape)\n",
    "\n",
    "    conv2 = information_block(pool1, 64, padding='same', activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('conv2', conv2.shape)\n",
    "    pool2 = pooling_block(inputs=conv2, filters=64, activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('pool2', pool2.shape)\n",
    "    pool2 = Dropout(0.5)(pool2)\n",
    "    print('pool2', pool2.shape)\n",
    "\n",
    "    conv3 = information_block(pool2, 128, padding='same', activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('conv3', conv3.shape)\n",
    "    pool3 = pooling_block(inputs=conv3, filters=128, activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('pool3', pool3.shape)\n",
    "    pool3 = Dropout(0.5)(pool3)\n",
    "    print('pool3', pool3.shape)\n",
    "\n",
    "    conv4 = information_block(pool3, 256, padding='same', activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('conv4', conv4.shape)\n",
    "    pool4 = pooling_block(inputs=conv4, filters=256, activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('pool4', pool4.shape)\n",
    "    pool4 = Dropout(0.5)(pool4)\n",
    "    print('pool4', pool4.shape)\n",
    "\n",
    "    #\n",
    "    # bottom level of the U-net\n",
    "    #\n",
    "    conv5 = information_block(pool4, 512, padding='same', activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('conv5', conv5.shape)\n",
    "    conv5 = Dropout(0.5)(conv5)\n",
    "    print('conv5', conv5.shape)\n",
    "\n",
    "                  \n",
    "                  \n",
    "    \n",
    "    print(tf.shape(conv5))\n",
    "    td=conv5\n",
    "    \n",
    "    tx=softmax(td,axis=[1,2])\n",
    "    q=1.31\n",
    "    \n",
    "    \n",
    "    print(\"value of q\",q)\n",
    "    \n",
    "    \n",
    "    \n",
    "    safe_x = K.maximum(tx,1e-6)\n",
    "    new_p = K.switch(K.equal(q,1.), (K.log(safe_x)*safe_x)/K.log(2.718),(K.pow(safe_x,q))/(q-1))\n",
    "    new_p = -new_p #-p^q/(q-1)\n",
    "    \n",
    "    if q!=1:\n",
    "        new_p= 1+new_p\n",
    "        \n",
    "    print(\"new_p\",new_p.shape)\n",
    "    xyz=K.max(new_p,axis=[1,2],keepdims=True)\n",
    "    print(\"xyz=\",xyz.shape)\n",
    "   \n",
    "    \n",
    "    print(\"xyz=\",xyz.shape)\n",
    "    weights=1-(new_p/xyz)\n",
    "    print(\"weights\",weights.shape)\n",
    "    improved_p=td*weights\n",
    "    print(\"improved_p=\",improved_p.shape)\n",
    "    improved_p=K.sum(improved_p,axis=[1,2])/144\n",
    "    print(\"improved_pv2=\",type(improved_p),improved_p.shape)\n",
    "\n",
    "\n",
    "    #dense1=Dense(32,activation='relu')(improved_p)\n",
    "    #print(\"dense1\",dense1)\n",
    "    dense2=Dense(1,activation='softmax')(improved_p)\n",
    "    print(\"below dense\",dense2)\n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "\n",
    "    after_conv4 = connection_block(conv4, 256, padding='same', activation='elu',\n",
    "                                   pars=pars, allowed_pars=allowed_pars)\n",
    "    print('after_conv4', after_conv4.shape)\n",
    "    up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), after_conv4], axis=3)\n",
    "    conv6 = information_block(up6, 256, padding='same', activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('conv6', conv6.shape)\n",
    "    conv6 = Dropout(0.5)(conv6)\n",
    "    print('conv6', conv6.shape)\n",
    "\n",
    "    after_conv3 = connection_block(conv3, 128, padding='same', activation='elu',\n",
    "                                   pars=pars, allowed_pars=allowed_pars)\n",
    "    print('after_conv3', after_conv3.shape)\n",
    "    up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), after_conv3], axis=3)\n",
    "    conv7 = information_block(up7, 128, padding='same', activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('conv7', conv7.shape)\n",
    "    conv7 = Dropout(0.5)(conv7)\n",
    "    print('conv7', conv7.shape)\n",
    "\n",
    "    after_conv2 = connection_block(conv2, 64, padding='same', activation='elu', pars=pars,\n",
    "                                   allowed_pars=allowed_pars)\n",
    "    print('after_conv2', after_conv2.shape)\n",
    "    up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), after_conv2], axis=3)\n",
    "    conv8 = information_block(up8, 64, padding='same', activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('conv8', conv8.shape)\n",
    "    conv8 = Dropout(0.5)(conv8)\n",
    "    print('conv8', conv8.shape)\n",
    "\n",
    "    after_conv1 = connection_block(conv1, 32, padding='same', activation='elu',\n",
    "                                   pars=pars, allowed_pars=allowed_pars)\n",
    "    print('after_conv1', after_conv1.shape)\n",
    "    up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), after_conv1], axis=3)\n",
    "    conv9 = information_block(up9, 32, padding='same', activation='elu', pars=pars, allowed_pars=allowed_pars)\n",
    "    print('conv9', conv9.shape)\n",
    "    conv9 = Dropout(0.5)(conv9)\n",
    "    print('conv9', conv9.shape)\n",
    "\n",
    "    # main output\n",
    "    conv10 = Conv2D(1, kernel_size=(1, 1), kernel_initializer='he_normal', activation='sigmoid', name='main_output')(\n",
    "        conv9)\n",
    "    print('conv10', conv10.shape)\n",
    "\n",
    "    # creating a model\n",
    "    # compiling the model\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=[conv10, dense2]) #edit here for Tsallis\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-5),\n",
    "                  loss=[dice_coef_loss, tf.keras.losses.BinaryCrossentropy()],\n",
    "                      metrics=[dice_coef,tf.keras.metrics.BinaryCrossentropy()],\n",
    "                      loss_weights=[1., 0.5])\n",
    "\n",
    "    return model\n",
    "                  \n",
    "\n",
    "\n",
    "\n",
    "def preprocess(imgs):\n",
    "    imgs_p = np.ndarray((imgs.shape[0], img_rows, img_cols), dtype=np.uint8)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        imgs_p[i] = resize(imgs[i], (img_cols, img_rows), preserve_range=True)\n",
    " \n",
    "    imgs_p = imgs_p[..., np.newaxis]\n",
    "    return imgs_p\n",
    "\n",
    "def train_and_predict():\n",
    "                  \n",
    "    BEST_PARS = {\n",
    "    'outputs': 2,\n",
    "    'activation': 'elu',\n",
    "    'pooling_block': {'trainable': True},\n",
    "    'information_block': {'inception': {'v2': 'b'}},\n",
    "    'connection_block': 'residual'}\n",
    "    \n",
    "    ALLOWED_PARS = {\n",
    "    'outputs': [1, 2],\n",
    "    'activation': ['elu', 'relu'],\n",
    "    'pooling_block': {\n",
    "        'trainable': [True, False]},\n",
    "    'information_block': {\n",
    "        'inception': {\n",
    "            'v1': ['a', 'b'],\n",
    "            'v2': ['a', 'b', 'c'],\n",
    "            'et': ['a', 'b']},\n",
    "        'convolution': {\n",
    "            'simple': ['not_normalized', 'normalized'],\n",
    "            'dilated': ['not_normalized', 'normalized']}},\n",
    "    'connection_block': ['not_residual', 'residual']}\n",
    "                \n",
    "    imgs_train,imgs_mask_train,imgs_value_train=load_train_data()\n",
    " \n",
    "    imgs_train = preprocess(imgs_train)\n",
    "    imgs_mask_train = preprocess(imgs_mask_train)\n",
    " \n",
    "    imgs_train = imgs_train.astype('float32')\n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    " \n",
    "    imgs_train -= mean\n",
    "    imgs_train /= std\n",
    " \n",
    "    imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "    imgs_mask_train /= 255.  # scale masks to [0, 1]\n",
    " \n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "    model = get_unet(BEST_PARS,ALLOWED_PARS)\n",
    "    model_checkpoint = ModelCheckpoint('weights.h5', monitor='val_loss' , mode = 'min' , verbose = 1 , save_best_only=True )\n",
    "    early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "    print('-'*30)\n",
    "    print('Fitting model...')\n",
    "    print('-'*30)\n",
    "    model.fit(imgs_train, y=[imgs_mask_train,imgs_value_train], batch_size=64, epochs= 40, verbose=1, shuffle=True,\n",
    "              validation_split=0.2 ,  callbacks=[model_checkpoint,early_stopping])\n",
    "    \n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing test data...')\n",
    "    print('-'*30)\n",
    "    imgs_test, imgs_id_test = load_test_data()\n",
    "    imgs_test = preprocess(imgs_test)\n",
    "\n",
    "    imgs_test = imgs_test.astype('float32')\n",
    "    imgs_test -= mean\n",
    "    imgs_test /= std\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Loading saved weights...')\n",
    "    print('-'*30)\n",
    "    model.load_weights('weights.h5')\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks on test data...')\n",
    "    print('-'*30)\n",
    "    n_imgs_test = imgs_test.shape[0]\n",
    "    #for i in range(n_imgs_test):\n",
    "\n",
    "    #worked for 7 vs 1\n",
    "    #imgs_mask_test, imgs_value_test= model.predict(np.expand_dims(imgs_test[0] , axis = 0), verbose=1)\n",
    "\n",
    "\n",
    "    imgs_mask_test, imgs_value_test= model.predict(imgs_test, verbose=1)\n",
    "    print(\"mask final img 1\" , imgs_mask_test)\n",
    "    np.save('imgs_mask_test.npy', imgs_mask_test)\n",
    "\n",
    "    print('-' * 30)\n",
    "    print('Saving predicted masks to files...')\n",
    "    print('-' * 30)\n",
    "    pred_dir = 'preds'\n",
    "    \n",
    "    # print(imgs_mask_test.shape , imgs_value_test.shape)\n",
    "    if not os.path.exists(pred_dir):\n",
    "        os.mkdir(pred_dir)\n",
    "    for image, image_id in zip(imgs_mask_test, imgs_id_test):\n",
    "        image = (image[:, :, 0] * 255.).astype(np.uint8)\n",
    "        imsave(os.path.join(pred_dir, str(image_id) + '_pred.png'), image)\n",
    "\n",
    "\n",
    " \n",
    "     \n",
    "if __name__ == '__main__':\n",
    "    train_and_predict()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef09d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
